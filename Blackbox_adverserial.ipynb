{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebeb6d9e-4c74-43e1-95b4-3b88b301666f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c14eb41-e53a-47b1-81b6-ff6d63e76394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.1.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.8.0.dev20250319+cu128)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.78.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (104 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.10.0)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.31.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.8.0.87 in /usr/local/lib/python3.11/dist-packages (from torch) (9.8.0.87)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /usr/local/lib/python3.11/dist-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /usr/local/lib/python3.11/dist-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.25.1 in /usr/local/lib/python3.11/dist-packages (from torch) (2.25.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /usr/local/lib/python3.11/dist-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: pytorch-triton==3.3.0+git96316ce5 in /usr/local/lib/python3.11/dist-packages (from torch) (3.3.0+git96316ce5)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-triton==3.3.0+git96316ce5->torch) (77.0.1)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.0 (from huggingface-hub>=0.24.0->datasets)\n",
      "  Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (494 bytes)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Downloading matplotlib-3.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading openai-1.78.1-py3-none-any.whl (680 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m680.9/680.9 kB\u001b[0m \u001b[31m134.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "Downloading contourpy-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fonttools-4.58.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.1-py3-none-any.whl (484 kB)\n",
      "Downloading jiter-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (351 kB)\n",
      "Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Downloading kiwisolver-1.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m333.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m303.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m216.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m336.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohttp-3.11.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m314.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading hf_xet-1.1.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (313 kB)\n",
      "Downloading multidict-6.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
      "Downloading propcache-0.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading yarl-1.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (358 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-inspection, tqdm, threadpoolctl, scipy, safetensors, regex, pydantic-core, pyarrow, propcache, multidict, kiwisolver, joblib, jiter, hf-xet, frozenlist, fonttools, dill, cycler, contourpy, annotated-types, aiohappyeyeballs, yarl, scikit-learn, pydantic, pandas, multiprocess, matplotlib, huggingface-hub, aiosignal, tokenizers, seaborn, openai, aiohttp, transformers, bitsandbytes, accelerate, datasets\n",
      "Successfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 bitsandbytes-0.45.5 contourpy-1.3.2 cycler-0.12.1 datasets-3.6.0 dill-0.3.8 fonttools-4.58.0 frozenlist-1.6.0 hf-xet-1.1.0 huggingface-hub-0.31.1 jiter-0.9.0 joblib-1.5.0 kiwisolver-1.4.8 matplotlib-3.10.3 multidict-6.4.3 multiprocess-0.70.16 openai-1.78.1 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pydantic-2.11.4 pydantic-core-2.33.2 pytz-2025.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.3 seaborn-0.13.2 threadpoolctl-3.6.0 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3 typing-inspection-0.4.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib numpy datasets transformers torch scikit-learn tqdm openai seaborn bitsandbytes accelerate\\>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e9d889-5142-40d1-b1aa-516e60a95ddf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Important: do the following in terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3c000b-e0d6-4812-a2b4-c16f4cb1a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone https://github.com/liamdugan/raid.git\n",
    "#cd raid\n",
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8f10e-a75d-418e-be4b-3abe70d60c56",
   "metadata": {},
   "source": [
    "# Hugginface Login (add your login key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec914506-4989-4e30-b5f7-e0312a7b1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=\"please_replace_with_your_key\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your Hugging Face token when prompted\n",
    "login(API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1075365-6703-4013-a922-2f9a13c7e39b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting raid-bench\n",
      "  Downloading raid_bench-0.1.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy~=1.26.4 (from raid-bench)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pandas~=2.2.2 in /usr/local/lib/python3.11/dist-packages (from raid-bench) (2.2.3)\n",
      "Collecting scikit-learn~=1.3.2 (from raid-bench)\n",
      "  Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from raid-bench) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.2->raid-bench) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.2->raid-bench) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas~=2.2.2->raid-bench) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.3.2->raid-bench) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.3.2->raid-bench) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.3.2->raid-bench) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas~=2.2.2->raid-bench) (1.16.0)\n",
      "Downloading raid_bench-0.1.0-py3-none-any.whl (11 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, scikit-learn, raid-bench\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "Successfully installed numpy-1.26.4 raid-bench-0.1.0 scikit-learn-1.3.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install raid-bench\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c0da82c-2b40-4caf-8686-4a164af2a044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from raid import run_detection, run_evaluation\n",
    "from raid.utils import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872c3835-c71f-48a1-afe9-904fecbdb661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available() and torch.cuda.get_device_capability(0) == (10, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acaa683-c2af-4a9e-bc77-33b8694f19b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83c22b57-30f2-451c-bbfe-ef4fa15c4d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://dataset.raid-bench.xyz/train.csv (11779491051B) to /root/.cache/raid/train.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11.8G/11.8G [03:15<00:00, 60.1MB/s]  \n"
     ]
    }
   ],
   "source": [
    "train_df = load_data(split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98635c9-fa16-4fa2-a13e-7ea5a18813d3",
   "metadata": {},
   "source": [
    "# Experiment for no adverserial attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0effdc87-bb00-47ea-8b83-7e2558b5c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_4=train_df[ (train_df['model'] == 'human') & (train_df['attack'] == 'none') ]\n",
    "train_df_4_sampled = train_df_4.sample(frac=1, random_state=42)\n",
    "train_df_3=train_df[  (train_df['model'] != 'human')  & (train_df[\"attack\"]==\"none\") ]# & (train_df['attack'] == 'none') ]#train_df[ (train_df['attack'] == 'synonym') ]# & (train_df['decoding'] == 'sampling') & (train_df['repetition_penalty'] == 'yes') ]\n",
    "train_df_3_sampled = train_df_3.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "#(train_df['repetition_penalty'] == 'yes')&\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "human_samples=train_df_4_sampled\n",
    "train_df_mmini=train_df_3_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "643f5527-a868-4c21-b1cc-ed4ac1bb7e00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794b9efc9a4044108ec130903c21f16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/5850 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "model_name = \"tiiuae/falcon-rw-1b\"#\"EleutherAI/gpt-neo-1.3B\"#\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "if NUM_GPUS > 1:\n",
    "    base_model = torch.nn.DataParallel(base_model)\n",
    "base_model.to(DEVICE)\n",
    "base_model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "# UPDATED my_detector for proper batching (this is critical!)\n",
    "def my_detector(texts: list[str]) -> list[float]:\n",
    "    with torch.no_grad():\n",
    "        tokenized = tokenizer(texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "        tokenized = {k: v.to(DEVICE) for k, v in tokenized.items()}\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "        outputs = base_model(**tokenized)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = tokenized[\"input_ids\"][:, 1:]\n",
    "\n",
    "        # Compute log-likelihood per sample manually\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        ll_per_token = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "        attention_mask = tokenized[\"attention_mask\"][:, 1:]\n",
    "        ll_per_sample = (ll_per_token * attention_mask).sum(dim=-1) / attention_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "        # Compute entropy per sample\n",
    "        #neg_entropy = F.softmax(logits, dim=-1) * log_probs\n",
    "        neg_entropy = (log_probs.exp() * log_probs)\n",
    "\n",
    "        entropy_per_sample = -(neg_entropy.sum(dim=-1) * attention_mask).sum(-1) / attention_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "        # Combine log-likelihood and entropy\n",
    "        scores = ( abs(entropy_per_sample+ll_per_sample)).cpu().tolist()#\n",
    "        \n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def batch_gpu_detector(all_texts):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(all_texts), BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "        batch_texts = all_texts[i:i + BATCH_SIZE]\n",
    "        results.extend(my_detector(batch_texts))\n",
    "    return results\n",
    "\n",
    "\n",
    "human_samples=train_df_4_sampled\n",
    "train_df_mmini=train_df_3_sampled\n",
    "\n",
    "# Combine human and AI samples\n",
    "train_df_mini = pd.concat([human_samples, train_df_mmini]).reset_index(drop=True)\n",
    "\n",
    "# Drop duplicates based on 'id'\n",
    "train_df_mini = train_df_mini.drop_duplicates(subset=\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run detection\n",
    "predictions = batch_gpu_detector(train_df_mini['generation'].tolist())\n",
    "\n",
    "# Prepare evaluation data correctly\n",
    "results_for_eval = [{\"id\": row[\"id\"], \"score\": score} for row, score in zip(train_df_mini.to_dict('records'), predictions)]\n",
    "\n",
    "# Evaluate\n",
    "evaluation_result = run_evaluation(results_for_eval, train_df_mini, target_fpr=0.05,require_complete=True, per_domain_tuning=True)\n",
    "scores_df = pd.DataFrame(evaluation_result['scores'])\n",
    "\n",
    "# Visualization\n",
    "#scores_df = pd.DataFrame(evaluation_result['scores'])\n",
    "#plt.figure(figsize=(12, 6))\n",
    "#sns.barplot(x='model', y='accuracy', hue='attack', data=scores_df)\n",
    "#plt.title('Accuracy by Model and Attack Type')\n",
    "#plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "233aa973-b4ea-412c-9f66-d9d73528b051",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.to_csv(\"scores_output_blackbox.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b767c65-b031-4eb4-a74d-3a9eb7cdb34f",
   "metadata": {},
   "source": [
    "# Table for no adverserial attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d04d7583-470c-4146-878e-7cfeda32174d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>tp</th>\n",
       "      <th>fn</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1580</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>none</td>\n",
       "      <td>375106</td>\n",
       "      <td>79508</td>\n",
       "      <td>0.825109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>375106</td>\n",
       "      <td>79508</td>\n",
       "      <td>0.825109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     domain model decoding repetition_penalty attack      tp     fn  accuracy\n",
       "1580    all   all      all                all   none  375106  79508  0.825109\n",
       "1673    all   all      all                all    all  375106  79508  0.825109"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"all\")&(scores_df[\"model\"]==\"all\")&(scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"all\") ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19db0a8-ea22-4415-b7c1-4e759e1af40f",
   "metadata": {},
   "source": [
    "## Results open source chatmodels (llama-c, mistral-c, mpt-c) - no penalty rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c70cdcb6-9bbd-4520-bcf6-4c94d9b4acf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9939670431032334"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"greedy\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"llama-chat\")|(scores_df[\"model\"]==\"mistral-chat\")|(scores_df[\"model\"]==\"mpt-chat\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"no\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "951df9a7-e902-4742-a4b0-b83eaad215cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9789345100092239"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"sampling\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"llama-chat\")|(scores_df[\"model\"]==\"mistral-chat\")|(scores_df[\"model\"]==\"mpt-chat\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"no\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63424c85-4ee5-4f4c-8d6f-3948a5615cac",
   "metadata": {},
   "source": [
    "## Results open source chatmodels (llama-c, mistral-c, mpt-c) - with penalty rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04818d84-c5ee-4ba9-9f1b-76d8ae908262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8638346670655398"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"greedy\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"llama-chat\")|(scores_df[\"model\"]==\"mistral-chat\")|(scores_df[\"model\"]==\"mpt-chat\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"yes\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78657d5e-78c1-4e8f-a42b-deeb84a80329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6219430109939421"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"sampling\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"llama-chat\")|(scores_df[\"model\"]==\"mistral-chat\")|(scores_df[\"model\"]==\"mpt-chat\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"yes\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f308bb-95d4-4089-b4cf-039c9743df57",
   "metadata": {},
   "source": [
    "## Results open source non-chatmodels (mistral, mpt, gpt2)- no penalty rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "908c7797-4741-4b53-8693-42c54fcaee22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9081345199810534"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"greedy\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"gpt2\")|(scores_df[\"model\"]==\"mistral\")|(scores_df[\"model\"]==\"mpt\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"no\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97c21a64-c3f0-431b-ac3a-6d3a926562fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5399995014085209"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"sampling\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"gpt2\")|(scores_df[\"model\"]==\"mistral\")|(scores_df[\"model\"]==\"mpt\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"no\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4a81d-41c9-4bee-b863-9b45853f415c",
   "metadata": {},
   "source": [
    "## Results open source non-chatmodels (mistral, mpt, gpt2)- with penalty rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93d0caff-30e7-4e4d-8b07-8f8de8be17d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8402512901054521"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"greedy\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"gpt2\")|(scores_df[\"model\"]==\"mistral\")|(scores_df[\"model\"]==\"mpt\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"yes\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd724806-2a7d-4d6b-b8ba-7354883e753d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7330291925310997"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"sampling\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"gpt2\")|(scores_df[\"model\"]==\"mistral\")|(scores_df[\"model\"]==\"mpt\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"yes\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfca82d-6e98-4b3a-9c7a-7f440bb2b4b1",
   "metadata": {},
   "source": [
    "## Results closed source chatmodels (chatgpt-chat, gpt4, cohere) - no penalty rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81220763-c4ca-4a01-86ed-a3bc8880590a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9642509909505647"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"greedy\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"chatgpt\")|(scores_df[\"model\"]==\"gpt4\")|(scores_df[\"model\"]==\"cohere-chat\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"no\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "972cd4b8-8d96-43d9-a064-407f5f4420ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7024655348640092"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"sampling\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"chatgpt\")|(scores_df[\"model\"]==\"gpt4\")|(scores_df[\"model\"]==\"cohere-chat\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"no\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d57b0-2282-4948-a1f2-c0770d3dc4ff",
   "metadata": {},
   "source": [
    "## Results closed source non-chatmodels (cohere, gpt3) - no penalty rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2f79408-810e-44dc-8a0a-ee0ea4e37a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9440954304090943"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"greedy\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"cohere\")|(scores_df[\"model\"]==\"gpt3\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"no\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d492c1d4-2545-4b6f-a7ea-124ac8adf113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8625383292199535"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"sampling\") & (scores_df[\"attack\"]==\"none\")\n",
    "            & ((scores_df[\"model\"]==\"cohere\")|(scores_df[\"model\"]==\"gpt3\")) \n",
    "            & (scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"no\")   ][\"accuracy\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60a0d76-5cb7-44e3-8962-adff9e171c8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Adverserial experiments and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "338367e7-10e6-4574-9434-81e2d1214a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_4=train_df[ (train_df['model'] == 'human') ]\n",
    "train_df_4_sampled = train_df_4.sample(frac=1, random_state=42)\n",
    "train_df_3=train_df[  (train_df['model'] != 'human')  ]# & (train_df['attack'] == 'none') ]#train_df[ (train_df['attack'] == 'synonym') ]# & (train_df['decoding'] == 'sampling') & (train_df['repetition_penalty'] == 'yes') ]\n",
    "train_df_3_sampled = train_df_3.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "#(train_df['repetition_penalty'] == 'yes')&\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "human_samples=train_df_4_sampled\n",
    "train_df_mmini=train_df_3_sampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "891c5974-2e53-43dc-ac05-005e7653737e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2a4593ba884456b140c1f63022f10e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Batches:   0%|          | 0/70198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "model_name = \"tiiuae/falcon-rw-1b\"#\"EleutherAI/gpt-neo-1.3B\"#\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "if NUM_GPUS > 1:\n",
    "    base_model = torch.nn.DataParallel(base_model)\n",
    "base_model.to(DEVICE)\n",
    "base_model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "# UPDATED my_detector for proper batching (this is critical!)\n",
    "def my_detector(texts: list[str]) -> list[float]:\n",
    "    with torch.no_grad():\n",
    "        tokenized = tokenizer(texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "        tokenized = {k: v.to(DEVICE) for k, v in tokenized.items()}\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "        outputs = base_model(**tokenized)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = tokenized[\"input_ids\"][:, 1:]\n",
    "\n",
    "        # Compute log-likelihood per sample manually\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        ll_per_token = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "        attention_mask = tokenized[\"attention_mask\"][:, 1:]\n",
    "        ll_per_sample = (ll_per_token * attention_mask).sum(dim=-1) / attention_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "        # Compute entropy per sample\n",
    "        #neg_entropy = F.softmax(logits, dim=-1) * log_probs\n",
    "        neg_entropy = (log_probs.exp() * log_probs)\n",
    "\n",
    "        entropy_per_sample = -(neg_entropy.sum(dim=-1) * attention_mask).sum(-1) / attention_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "        # Combine log-likelihood and entropy\n",
    "        scores = ( abs(entropy_per_sample+ll_per_sample)).cpu().tolist()#\n",
    "        \n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def batch_gpu_detector(all_texts):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(all_texts), BATCH_SIZE), desc=\"Processing Batches\"):\n",
    "        batch_texts = all_texts[i:i + BATCH_SIZE]\n",
    "        results.extend(my_detector(batch_texts))\n",
    "    return results\n",
    "\n",
    "\n",
    "human_samples=train_df_4_sampled\n",
    "train_df_mmini=train_df_3_sampled\n",
    "\n",
    "# Combine human and AI samples\n",
    "train_df_mini = pd.concat([human_samples, train_df_mmini]).reset_index(drop=True)\n",
    "\n",
    "# Drop duplicates based on 'id'\n",
    "train_df_mini = train_df_mini.drop_duplicates(subset=\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run detection\n",
    "predictions = batch_gpu_detector(train_df_mini['generation'].tolist())\n",
    "\n",
    "# Prepare evaluation data correctly\n",
    "results_for_eval = [{\"id\": row[\"id\"], \"score\": score} for row, score in zip(train_df_mini.to_dict('records'), predictions)]\n",
    "\n",
    "# Evaluate\n",
    "evaluation_result = run_evaluation(results_for_eval, train_df_mini, target_fpr=0.05,require_complete=True, per_domain_tuning=True)\n",
    "scores_df = pd.DataFrame(evaluation_result['scores'])\n",
    "\n",
    "# Visualization\n",
    "\n",
    "#plt.figure(figsize=(12, 6))\n",
    "#sns.barplot(x='model', y='accuracy', hue='attack', data=scores_df)\n",
    "#plt.title('Accuracy by Model and Attack Type')\n",
    "#plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54acbd0-a9b5-40c3-9398-169c8849bb0d",
   "metadata": {},
   "source": [
    "## Table for adverserial attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e5666c8-7e68-4b66-97a6-5c0dd648d463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>tp</th>\n",
       "      <th>fn</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9764</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>whitespace</td>\n",
       "      <td>276361</td>\n",
       "      <td>178253</td>\n",
       "      <td>0.607903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9857</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>upper_lower</td>\n",
       "      <td>329478</td>\n",
       "      <td>125136</td>\n",
       "      <td>0.724742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9950</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>article_deletion</td>\n",
       "      <td>338343</td>\n",
       "      <td>116271</td>\n",
       "      <td>0.744242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10043</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>none</td>\n",
       "      <td>375106</td>\n",
       "      <td>79508</td>\n",
       "      <td>0.825109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10136</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>alternative_spelling</td>\n",
       "      <td>366566</td>\n",
       "      <td>88048</td>\n",
       "      <td>0.806324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10229</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>perplexity_misspelling</td>\n",
       "      <td>362713</td>\n",
       "      <td>91901</td>\n",
       "      <td>0.797848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10322</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>insert_paragraphs</td>\n",
       "      <td>368468</td>\n",
       "      <td>86146</td>\n",
       "      <td>0.810507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10508</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>number</td>\n",
       "      <td>340254</td>\n",
       "      <td>114360</td>\n",
       "      <td>0.748446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10601</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>paraphrase</td>\n",
       "      <td>337331</td>\n",
       "      <td>117283</td>\n",
       "      <td>0.742016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10694</th>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>all</td>\n",
       "      <td>synonym</td>\n",
       "      <td>221025</td>\n",
       "      <td>233589</td>\n",
       "      <td>0.486182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      domain model decoding repetition_penalty                  attack  \\\n",
       "9764     all   all      all                all              whitespace   \n",
       "9857     all   all      all                all             upper_lower   \n",
       "9950     all   all      all                all        article_deletion   \n",
       "10043    all   all      all                all                    none   \n",
       "10136    all   all      all                all    alternative_spelling   \n",
       "10229    all   all      all                all  perplexity_misspelling   \n",
       "10322    all   all      all                all       insert_paragraphs   \n",
       "10508    all   all      all                all                  number   \n",
       "10601    all   all      all                all              paraphrase   \n",
       "10694    all   all      all                all                 synonym   \n",
       "\n",
       "           tp      fn  accuracy  \n",
       "9764   276361  178253  0.607903  \n",
       "9857   329478  125136  0.724742  \n",
       "9950   338343  116271  0.744242  \n",
       "10043  375106   79508  0.825109  \n",
       "10136  366566   88048  0.806324  \n",
       "10229  362713   91901  0.797848  \n",
       "10322  368468   86146  0.810507  \n",
       "10508  340254  114360  0.748446  \n",
       "10601  337331  117283  0.742016  \n",
       "10694  221025  233589  0.486182  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"all\")&(scores_df[\"model\"]==\"all\")&(scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"all\") & (scores_df[\"attack\"]!=\"homoglyph\") & (scores_df[\"attack\"]!=\"all\") &(scores_df[\"attack\"]!=\"zero_width_space\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0a87b0b-bf08-45bc-9495-b586f0b74962",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.to_csv(\"scores_output_adv.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047e74b2-71db-413d-9e6b-8e1c20887153",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Adverserial with preprocessing if you wish to remove homoglyph and zero width space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5834522-3668-4b36-b79d-48e7149657d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_4=train_df[ (train_df['model'] == 'human') ]\n",
    "train_df_4_sampled = train_df_4.sample(frac=0.05, random_state=42)\n",
    "train_df_3=train_df[  (train_df['model'] != 'human')  ]# & (train_df['attack'] == 'none') ]#train_df[ (train_df['attack'] == 'synonym') ]# & (train_df['decoding'] == 'sampling') & (train_df['repetition_penalty'] == 'yes') ]\n",
    "train_df_3_sampled = train_df_3.sample(frac=0.003, random_state=42)\n",
    "\n",
    "\n",
    "#(train_df['repetition_penalty'] == 'yes')&\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "human_samples=train_df_4_sampled\n",
    "train_df_mmini=train_df_3_sampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf08eb-7cc3-49ce-b35f-862fc50fb865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Extended manual homoglyph map (non-Latin to Latin equivalents)\n",
    "HOMOGLYPH_MAP = {\n",
    "    # Cyrillic\n",
    "    'а': 'a', 'А': 'A', 'в': 'b', 'В': 'B', 'е': 'e', 'Е': 'E',\n",
    "    'і': 'i', 'І': 'I', 'ј': 'j', 'Ј': 'J', 'к': 'k', 'К': 'K',\n",
    "    'м': 'm', 'М': 'M', 'н': 'h', 'Н': 'H', 'о': 'o', 'О': 'O',\n",
    "    'р': 'p', 'Р': 'P', 'с': 'c', 'С': 'C', 'т': 't', 'Т': 'T',\n",
    "    'у': 'y', 'У': 'Y', 'х': 'x', 'Х': 'X', 'ч': 'ch', 'Ч': 'Ch',\n",
    "    'ѕ': 's', 'Ѕ': 'S', 'ъ': 'y', 'Ъ': 'Y', 'ь': '', 'Ь': '',\n",
    "    # Greek\n",
    "    'α': 'a', 'Α': 'A', 'β': 'b', 'Β': 'B', 'γ': 'g', 'Γ': 'G',\n",
    "    'ε': 'e', 'Ε': 'E', 'η': 'h', 'Η': 'H', 'ι': 'i', 'Ι': 'I',\n",
    "    'κ': 'k', 'Κ': 'K', 'ν': 'n', 'Ν': 'N', 'ο': 'o', 'Ο': 'O',\n",
    "    'ρ': 'p', 'Ρ': 'P', 'σ': 's', 'Σ': 'S', 'τ': 't', 'Τ': 'T',\n",
    "    'χ': 'x', 'Χ': 'X', 'ζ': 'z', 'Ζ': 'Z',\n",
    "    # Other confusable characters\n",
    "    'ꞗ': 'r', 'Ꞗ': 'R', 'ꭓ': 'x', 'ꭔ': 'u', 'ꭕ': 'u', 'ꭖ': 'y',\n",
    "    'ʍ': 'w', 'ᴡ': 'W', 'ı': 'i', 'ɪ': 'I',\n",
    "    # Additional scripts (e.g., Armenian, Hebrew)\n",
    "    'ա': 'w', 'Ա': 'A', 'օ': 'o', 'Օ': 'O', '׳': '\\''\n",
    "}\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text by removing invisible characters, replacing homoglyphs with Latin equivalents,\n",
    "    and cleaning up whitespace. Designed to defend against homoglyph spoofing attacks.\n",
    "    \"\"\"\n",
    "    # Step 1: Remove zero-width and invisible characters\n",
    "    text = re.sub(r'[\\u200B-\\u200D\\uFEFF\\u2060\\u180E]', '', text)\n",
    "\n",
    "    # Step 2: Normalize Unicode (e.g., full-width characters, ligatures)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # Step 3: Replace homoglyphs with Latin equivalents\n",
    "    text = ''.join(HOMOGLYPH_MAP.get(char, char) for char in text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa5fc087-eca1-402e-b51f-5e28f7a99416",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply text preprocessing to neutralize attacks\n",
    "train_df_3_sampled['generation'] = train_df_3_sampled['generation'].apply(clean_text)\n",
    "train_df_4_sampled['generation'] = train_df_4_sampled['generation'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f843a8-2d41-418d-b2c8-bb9ea795630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"CPU\"\n",
    "\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "\n",
    "model_name = \"tiiuae/falcon-rw-1b\"#\"EleutherAI/gpt-neo-1.3B\"#\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "\n",
    "if NUM_GPUS > 1:\n",
    "    base_model = torch.nn.DataParallel(base_model)\n",
    "base_model.to(DEVICE)\n",
    "base_model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LENGTH = 512\n",
    "BATCH_SIZE = 80\n",
    "\n",
    "# UPDATED my_detector for proper batching (this is critical!)\n",
    "def my_detector(texts: list[str]) -> list[float]:\n",
    "    with torch.no_grad():\n",
    "        tokenized = tokenizer(texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n",
    "        tokenized = {k: v.to(DEVICE) for k, v in tokenized.items()}\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "\n",
    "        outputs = base_model(**tokenized)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = tokenized[\"input_ids\"][:, 1:]\n",
    "\n",
    "        # Compute log-likelihood per sample manually\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        ll_per_token = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "        attention_mask = tokenized[\"attention_mask\"][:, 1:]\n",
    "        ll_per_sample = (ll_per_token * attention_mask).sum(dim=-1) / attention_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "        # Compute entropy per sample\n",
    "        #neg_entropy = F.softmax(logits, dim=-1) * log_probs\n",
    "        neg_entropy = (log_probs.exp() * log_probs)\n",
    "\n",
    "        entropy_per_sample = -(neg_entropy.sum(dim=-1) * attention_mask).sum(-1) / attention_mask.sum(dim=1).clamp(min=1)\n",
    "\n",
    "        # Combine log-likelihood and entropy\n",
    "        scores = ( abs(entropy_per_sample+ll_per_sample)).cpu().tolist()#\n",
    "        \n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def batch_gpu_detector(all_texts):\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(all_texts), BATCH_SIZE), desc=\"Processing Batches\", file=sys.stdout):\n",
    "        batch_texts = all_texts[i:i + BATCH_SIZE]\n",
    "        results.extend(my_detector(batch_texts))\n",
    "    return results\n",
    "\n",
    "\n",
    "human_samples=train_df_4_sampled\n",
    "train_df_mmini=train_df_3_sampled\n",
    "\n",
    "# Combine human and AI samples\n",
    "train_df_mini = pd.concat([human_samples, train_df_mmini]).reset_index(drop=True)\n",
    "\n",
    "# Drop duplicates based on 'id'\n",
    "train_df_mini = train_df_mini.drop_duplicates(subset=\"id\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run detection\n",
    "predictions = batch_gpu_detector(train_df_mini['generation'].tolist())\n",
    "\n",
    "# Prepare evaluation data correctly\n",
    "results_for_eval = [{\"id\": row[\"id\"], \"score\": score} for row, score in zip(train_df_mini.to_dict('records'), predictions)]\n",
    "\n",
    "# Evaluate\n",
    "evaluation_result = run_evaluation(results_for_eval, train_df_mini, target_fpr=0.05,require_complete=True, per_domain_tuning=True)\n",
    "\n",
    "\n",
    "# Visualization\n",
    "scores_df = pd.DataFrame(evaluation_result['scores'])\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='model', y='accuracy', hue='attack', data=scores_df)\n",
    "plt.title('Accuracy by Model and Attack Type')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4058a-5246-4e66-bf6f-46138402b6cb",
   "metadata": {},
   "source": [
    "## table for adverserial with preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed16003-a1f0-4864-8a0e-c44cfdeb3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df[(scores_df[\"decoding\"]==\"all\")&(scores_df[\"model\"]==\"all\")&(scores_df[\"domain\"]==\"all\") & (scores_df[\"repetition_penalty\"]==\"all\") ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
