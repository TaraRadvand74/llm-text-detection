{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebeb6d9e-4c74-43e1-95b4-3b88b301666f",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c14eb41-e53a-47b1-81b6-ff6d63e76394",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.77.0-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Collecting accelerate>=0.26.0\n",
      "  Downloading accelerate-1.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.5/102.5 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2023.4.0)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (4.0.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
      "Collecting typing-extensions (from torch)\n",
      "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0) (5.9.6)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\n",
      "  Downloading huggingface_hub-0.30.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.1.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (1.26.13)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading matplotlib-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m139.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m173.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m281.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m289.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading openai-1.77.0-py3-none-any.whl (662 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.0/662.0 kB\u001b[0m \u001b[31m223.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-1.6.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.7/354.7 kB\u001b[0m \u001b[31m161.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m152.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fonttools-4.57.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m320.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.4/481.4 kB\u001b[0m \u001b[31m204.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m311.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiter-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.9/352.9 kB\u001b[0m \u001b[31m164.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m154.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m311.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m230.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.9/443.9 kB\u001b[0m \u001b[31m166.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m302.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m226.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m169.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m226.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m300.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m308.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.3/287.3 kB\u001b[0m \u001b[31m112.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.8/219.8 kB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.6/206.6 kB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m187.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m164.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.9/333.9 kB\u001b[0m \u001b[31m134.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, typing-extensions, tqdm, threadpoolctl, tabulate, scipy, safetensors, requests, regex, pyarrow, propcache, kiwisolver, joblib, jiter, h11, fsspec, frozenlist, fonttools, dill, cycler, contourpy, async-timeout, annotated-types, aiohappyeyeballs, typing-inspection, scikit-learn, pydantic-core, pandas, multiprocess, multidict, matplotlib, huggingface-hub, httpcore, aiosignal, yarl, tokenizers, pydantic, httpx, bitsandbytes, accelerate, transformers, openai, aiohttp, datasets\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-1.6.0 aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 bitsandbytes-0.45.5 contourpy-1.3.2 cycler-0.12.1 datasets-3.5.1 dill-0.3.8 fonttools-4.57.0 frozenlist-1.6.0 fsspec-2025.3.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 huggingface-hub-0.30.2 jiter-0.9.0 joblib-1.5.0 kiwisolver-1.4.8 matplotlib-3.10.1 multidict-6.4.3 multiprocess-0.70.16 openai-1.77.0 pandas-2.2.3 propcache-0.3.1 pyarrow-20.0.0 pydantic-2.11.4 pydantic-core-2.33.2 pytz-2025.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.2 tabulate-0.9.0 threadpoolctl-3.6.0 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.51.3 typing-extensions-4.13.2 typing-inspection-0.4.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib numpy datasets transformers torch scikit-learn tqdm tabulate openai bitsandbytes accelerate\\>=0.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8f10e-a75d-418e-be4b-3abe70d60c56",
   "metadata": {},
   "source": [
    "# Hugginface Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec914506-4989-4e30-b5f7-e0312a7b1589",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY=\"please_replace_with_your_key\"\n",
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your Hugging Face token when prompted\n",
    "login(API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907cc7c9-7077-4f1b-872d-3a8fc0cc6b7d",
   "metadata": {},
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3367113d-278d-46dd-b118-a5fee8958a57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datasets\n",
    "import transformers\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import random\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import json\n",
    "import functools\n",
    "import custom_datasets\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Generate a repeated version of the tab10 color map for 20 colors\n",
    "COLORS = [plt.get_cmap('tab20')(i % 20) for i in range(20)]\n",
    "\n",
    "\n",
    "# define regex to match all <extra_id_*> tokens, where * is an integer\n",
    "pattern = re.compile(r\"<extra_id_\\d+>\")\n",
    "\n",
    "\n",
    "def load_base_model():\n",
    "    print('MOVING BASE MODEL TO GPU...', end='', flush=True)\n",
    "    start = time.time()\n",
    "    try:\n",
    "        mask_model.cpu()\n",
    "    except NameError:\n",
    "        pass\n",
    "    if args.openai_model is None:\n",
    "        base_model.to(DEVICE)\n",
    "    print(f'DONE ({time.time() - start:.2f}s)')\n",
    "\n",
    "\n",
    "def load_mask_model():\n",
    "    print('MOVING MASK MODEL TO GPU...', end='', flush=True)\n",
    "    start = time.time()\n",
    "\n",
    "    if args.openai_model is None:\n",
    "        base_model.cpu()\n",
    "    if not args.random_fills:\n",
    "        mask_model.to(DEVICE)\n",
    "    print(f'DONE ({time.time() - start:.2f}s)')\n",
    "\n",
    "\n",
    "def tokenize_and_mask(text, span_length, pct, ceil_pct=False):\n",
    "    tokens = text.split(' ')\n",
    "    mask_string = '<<<mask>>>'\n",
    "\n",
    "    n_spans = pct * len(tokens) / (span_length + args.buffer_size * 2)\n",
    "    if ceil_pct:\n",
    "        n_spans = np.ceil(n_spans)\n",
    "    n_spans = int(n_spans)\n",
    "\n",
    "    n_masks = 0\n",
    "    while n_masks < n_spans:\n",
    "        start = np.random.randint(0, len(tokens) - span_length)\n",
    "        end = start + span_length\n",
    "        search_start = max(0, start - args.buffer_size)\n",
    "        search_end = min(len(tokens), end + args.buffer_size)\n",
    "        if mask_string not in tokens[search_start:search_end]:\n",
    "            tokens[start:end] = [mask_string]\n",
    "            n_masks += 1\n",
    "    \n",
    "    # replace each occurrence of mask_string with <extra_id_NUM>, where NUM increments\n",
    "    num_filled = 0\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if token == mask_string:\n",
    "            tokens[idx] = f'<extra_id_{num_filled}>'\n",
    "            num_filled += 1\n",
    "    assert num_filled == n_masks, f\"num_filled {num_filled} != n_masks {n_masks}\"\n",
    "    text = ' '.join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    "def count_masks(texts):\n",
    "    return [len([x for x in text.split() if x.startswith(\"<extra_id_\")]) for text in texts]\n",
    "\n",
    "\n",
    "# replace each masked span with a sample from T5 mask_model\n",
    "def replace_masks(texts):\n",
    "    n_expected = count_masks(texts)\n",
    "    stop_id = mask_tokenizer.encode(f\"<extra_id_{max(n_expected)}>\")[0]\n",
    "    tokens = mask_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    outputs = mask_model.generate(**tokens, max_length=150, do_sample=True, top_p=args.mask_top_p, num_return_sequences=1, eos_token_id=stop_id)\n",
    "    return mask_tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "\n",
    "\n",
    "def extract_fills(texts):\n",
    "    # remove <pad> from beginning of each text\n",
    "    texts = [x.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip() for x in texts]\n",
    "\n",
    "    # return the text in between each matched mask token\n",
    "    extracted_fills = [pattern.split(x)[1:-1] for x in texts]\n",
    "\n",
    "    # remove whitespace around each fill\n",
    "    extracted_fills = [[y.strip() for y in x] for x in extracted_fills]\n",
    "\n",
    "    return extracted_fills\n",
    "\n",
    "\n",
    "def apply_extracted_fills(masked_texts, extracted_fills):\n",
    "    # split masked text into tokens, only splitting on spaces (not newlines)\n",
    "    tokens = [x.split(' ') for x in masked_texts]\n",
    "\n",
    "    n_expected = count_masks(masked_texts)\n",
    "\n",
    "    # replace each mask token with the corresponding fill\n",
    "    for idx, (text, fills, n) in enumerate(zip(tokens, extracted_fills, n_expected)):\n",
    "        if len(fills) < n:\n",
    "            tokens[idx] = []\n",
    "        else:\n",
    "            for fill_idx in range(n):\n",
    "                text[text.index(f\"<extra_id_{fill_idx}>\")] = fills[fill_idx]\n",
    "\n",
    "    # join tokens back into text\n",
    "    texts = [\" \".join(x) for x in tokens]\n",
    "    return texts\n",
    "\n",
    "\n",
    "def perturb_texts_(texts, span_length, pct, ceil_pct=False):\n",
    "    if not args.random_fills:\n",
    "        masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for x in texts]\n",
    "        raw_fills = replace_masks(masked_texts)\n",
    "        extracted_fills = extract_fills(raw_fills)\n",
    "        perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "\n",
    "        # Handle the fact that sometimes the model doesn't generate the right number of fills and we have to try again\n",
    "        attempts = 1\n",
    "        while '' in perturbed_texts:\n",
    "            idxs = [idx for idx, x in enumerate(perturbed_texts) if x == '']\n",
    "            print(f'WARNING: {len(idxs)} texts have no fills. Trying again [attempt {attempts}].')\n",
    "            masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for idx, x in enumerate(texts) if idx in idxs]\n",
    "            raw_fills = replace_masks(masked_texts)\n",
    "            extracted_fills = extract_fills(raw_fills)\n",
    "            new_perturbed_texts = apply_extracted_fills(masked_texts, extracted_fills)\n",
    "            for idx, x in zip(idxs, new_perturbed_texts):\n",
    "                perturbed_texts[idx] = x\n",
    "            attempts += 1\n",
    "    else:\n",
    "        if args.random_fills_tokens:\n",
    "            # tokenize base_tokenizer\n",
    "            tokens = base_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "            valid_tokens = tokens.input_ids != base_tokenizer.pad_token_id\n",
    "            replace_pct = args.pct_words_masked * (args.span_length / (args.span_length + 2 * args.buffer_size))\n",
    "\n",
    "            # replace replace_pct of input_ids with random tokens\n",
    "            random_mask = torch.rand(tokens.input_ids.shape, device=DEVICE) < replace_pct\n",
    "            random_mask &= valid_tokens\n",
    "            random_tokens = torch.randint(0, base_tokenizer.vocab_size, (random_mask.sum(),), device=DEVICE)\n",
    "            # while any of the random tokens are special tokens, replace them with random non-special tokens\n",
    "            while any(base_tokenizer.decode(x) in base_tokenizer.all_special_tokens for x in random_tokens):\n",
    "                random_tokens = torch.randint(0, base_tokenizer.vocab_size, (random_mask.sum(),), device=DEVICE)\n",
    "            tokens.input_ids[random_mask] = random_tokens\n",
    "            perturbed_texts = base_tokenizer.batch_decode(tokens.input_ids, skip_special_tokens=True)\n",
    "        else:\n",
    "            masked_texts = [tokenize_and_mask(x, span_length, pct, ceil_pct) for x in texts]\n",
    "            perturbed_texts = masked_texts\n",
    "            # replace each <extra_id_*> with args.span_length random words from FILL_DICTIONARY\n",
    "            for idx, text in enumerate(perturbed_texts):\n",
    "                filled_text = text\n",
    "                for fill_idx in range(count_masks([text])[0]):\n",
    "                    fill = random.sample(FILL_DICTIONARY, span_length)\n",
    "                    filled_text = filled_text.replace(f\"<extra_id_{fill_idx}>\", \" \".join(fill))\n",
    "                assert count_masks([filled_text])[0] == 0, \"Failed to replace all masks\"\n",
    "                perturbed_texts[idx] = filled_text\n",
    "\n",
    "    return perturbed_texts\n",
    "\n",
    "\n",
    "def perturb_texts(texts, span_length, pct, ceil_pct=False):\n",
    "    chunk_size = args.chunk_size\n",
    "    if '11b' in mask_filling_model_name:\n",
    "        chunk_size //= 2\n",
    "\n",
    "    outputs = []\n",
    "    for i in tqdm.tqdm(range(0, len(texts), chunk_size), desc=\"Applying perturbations\"):\n",
    "        outputs.extend(perturb_texts_(texts[i:i + chunk_size], span_length, pct, ceil_pct=ceil_pct))\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def drop_last_word(text):\n",
    "    return ' '.join(text.split(' ')[:-1])\n",
    "\n",
    "\n",
    "def _openai_sample(p):\n",
    "    if args.dataset != 'pubmed':  # keep Answer: prefix for pubmed\n",
    "        p = drop_last_word(p)\n",
    "\n",
    "    # sample from the openai model\n",
    "    kwargs = { \"engine\": args.openai_model, \"max_tokens\": 200 }\n",
    "    if args.do_top_p:\n",
    "        kwargs['top_p'] = args.top_p\n",
    "    \n",
    "    r = openai.Completion.create(prompt=f\"{p}\", **kwargs)\n",
    "    return p + r['choices'][0].text\n",
    "\n",
    "\n",
    "# sample from base_model using ****only**** the first 30 tokens in each example as context\n",
    "def sample_from_model(texts, min_words=55, prompt_tokens=30):\n",
    "    # encode each text as a list of token ids\n",
    "    if args.dataset == 'pubmed':\n",
    "        texts = [t[:t.index(custom_datasets.SEPARATOR)] for t in texts]\n",
    "        all_encoded = base_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    else:\n",
    "        all_encoded = base_tokenizer(texts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        all_encoded = {key: value[:, :prompt_tokens] for key, value in all_encoded.items()}\n",
    "\n",
    "    if args.openai_model:\n",
    "        # decode the prefixes back into text\n",
    "        prefixes = base_tokenizer.batch_decode(all_encoded['input_ids'], skip_special_tokens=True)\n",
    "        pool = ThreadPool(args.batch_size)\n",
    "\n",
    "        decoded = pool.map(_openai_sample, prefixes)\n",
    "    else:\n",
    "        decoded = ['' for _ in range(len(texts))]\n",
    "\n",
    "        # sample from the model until we get a sample with at least min_words words for each example\n",
    "        # this is an inefficient way to do this (since we regenerate for all inputs if just one is too short), but it works\n",
    "        tries = 0\n",
    "        while (m := min(len(x.split()) for x in decoded)) < min_words:\n",
    "            if tries != 0:\n",
    "                print()\n",
    "                print(f\"min words: {m}, needed {min_words}, regenerating (try {tries})\")\n",
    "\n",
    "            sampling_kwargs = {}\n",
    "            if args.do_top_p:\n",
    "                sampling_kwargs['top_p'] = args.top_p\n",
    "            elif args.do_top_k:\n",
    "                sampling_kwargs['top_k'] = args.top_k\n",
    "            min_length = 50 if args.dataset in ['pubmed'] else 150\n",
    "            outputs = base_model.generate(**all_encoded, min_length=min_length, max_length=200, do_sample=True, **sampling_kwargs, pad_token_id=base_tokenizer.eos_token_id, eos_token_id=base_tokenizer.eos_token_id)\n",
    "            decoded = base_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            tries += 1\n",
    "\n",
    "    if args.openai_model:\n",
    "        global API_TOKEN_COUNTER\n",
    "\n",
    "        # count total number of tokens with GPT2_TOKENIZER\n",
    "        total_tokens = sum(len(GPT2_TOKENIZER.encode(x)) for x in decoded)\n",
    "        API_TOKEN_COUNTER += total_tokens\n",
    "\n",
    "    return decoded\n",
    "\n",
    "\n",
    "def get_likelihood(logits, labels):\n",
    "    assert logits.shape[0] == 1\n",
    "    assert labels.shape[0] == 1\n",
    "\n",
    "    logits = logits.view(-1, logits.shape[-1])[:-1]\n",
    "    labels = labels.view(-1)[1:]\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    log_likelihood = log_probs.gather(dim=-1, index=labels.unsqueeze(-1)).squeeze(-1)\n",
    "    return log_likelihood.mean()\n",
    "\n",
    "\n",
    "\n",
    "def get_ll(text):\n",
    "    with torch.no_grad():\n",
    "        tokenized = base_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        labels = tokenized.input_ids\n",
    "        outputs = base_model(**tokenized)\n",
    "        logits = outputs.logits[:, :-1, :]\n",
    "        labels = labels[:, 1:]  # Shift labels for autoregressive modeling\n",
    "        \n",
    "        # Compute log-likelihood manually\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        ll_per_token = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "        # Mask out padding tokens\n",
    "        attention_mask = tokenized.attention_mask[:, 1:]\n",
    "        valid_tokens = attention_mask.sum(dim=1)  # Count non-padding tokens\n",
    "        ll_per_sample = (ll_per_token * attention_mask).sum(dim=1) / valid_tokens  # Normalize\n",
    "\n",
    "        return ll_per_sample.mean().item()  # Average over batch\n",
    "\n",
    "\n",
    "\n",
    "def get_lls(texts):\n",
    "    if not args.openai_model:\n",
    "        return [get_ll(text) for text in texts]\n",
    "    else:\n",
    "        global API_TOKEN_COUNTER\n",
    "\n",
    "        # use GPT2_TOKENIZER to get total number of tokens\n",
    "        total_tokens = sum(len(GPT2_TOKENIZER.encode(text)) for text in texts)\n",
    "        API_TOKEN_COUNTER += total_tokens * 2  # multiply by two because OpenAI double-counts echo_prompt tokens\n",
    "\n",
    "        pool = ThreadPool(args.batch_size)\n",
    "        return pool.map(get_ll, texts)\n",
    "\n",
    "\n",
    "# get the average rank of each observed token sorted by model likelihood\n",
    "def get_rank(text, log=False):\n",
    "    assert args.openai_model is None, \"get_rank not implemented for OpenAI models\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokenized = base_tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
    "        logits = base_model(**tokenized).logits[:,:-1]\n",
    "        labels = tokenized.input_ids[:,1:]\n",
    "\n",
    "        # get rank of each label token in the model's likelihood ordering\n",
    "        matches = (logits.argsort(-1, descending=True) == labels.unsqueeze(-1)).nonzero()\n",
    "\n",
    "        assert matches.shape[1] == 3, f\"Expected 3 dimensions in matches tensor, got {matches.shape}\"\n",
    "\n",
    "        ranks, timesteps = matches[:,-1], matches[:,-2]\n",
    "\n",
    "        # make sure we got exactly one match for each timestep in the sequence\n",
    "        assert (timesteps == torch.arange(len(timesteps)).to(timesteps.device)).all(), \"Expected one match per timestep\"\n",
    "\n",
    "        ranks = ranks.float() + 1 # convert to 1-indexed rank\n",
    "        if log:\n",
    "            ranks = torch.log(ranks)\n",
    "\n",
    "        return ranks.float().mean().item()\n",
    "\n",
    "\n",
    "# get average entropy of each token in the text\n",
    "def get_entropy(text):\n",
    "    with torch.no_grad():\n",
    "        tokenized = base_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "        logits = base_model(**tokenized).logits[:, :-1, :]\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        entropy_per_token = -(probs * log_probs).sum(dim=-1)\n",
    "\n",
    "        # Mask padding tokens\n",
    "        attention_mask = tokenized.attention_mask[:, 1:]\n",
    "        valid_tokens = attention_mask.sum(dim=1)  # Count non-padding tokens\n",
    "        entropy_per_sample = (entropy_per_token * attention_mask).sum(dim=1) / valid_tokens  # Normalize\n",
    "\n",
    "        return entropy_per_sample.mean().item()\n",
    "\n",
    "\n",
    "def get_roc_metrics(real_preds, sample_preds, target_fpr=0.01):\n",
    "    fpr, tpr, _ = roc_curve([0] * len(real_preds) + [1] * len(sample_preds), real_preds + sample_preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Find the TPR at the given target FPR\n",
    "    idx = np.where(fpr <= target_fpr)[0][-1]  # Last index where FPR is below threshold\n",
    "    tp_at_low_fpr = tpr[idx]\n",
    "\n",
    "    \n",
    "    return fpr.tolist(), tpr.tolist(), float(roc_auc), float(tp_at_low_fpr)\n",
    "\n",
    "\n",
    "def get_precision_recall_metrics(real_preds, sample_preds):\n",
    "    precision, recall, _ = precision_recall_curve([0] * len(real_preds) + [1] * len(sample_preds), real_preds + sample_preds)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    return precision.tolist(), recall.tolist(), float(pr_auc)\n",
    "\n",
    "\n",
    "# save the ROC curve for each experiment, given a list of output dictionaries, one for each experiment, using colorblind-friendly colors\n",
    "# save the ROC curve for each experiment, given a list of output dictionaries, one for each experiment, using colorblind-friendly colors\n",
    "def save_roc_curves(experiments):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.clf()\n",
    "\n",
    "    # Define lists of line styles and markers to use cyclically\n",
    "    line_styles = ['--']#['-', '--', '-.', ':']\n",
    "    markers = ['o', 's', 'D', '^', 'v', 'P', '*', 'X', 'H', 'p', '>']  # A variety of marker styles\n",
    "\n",
    "    for idx, (experiment, color) in enumerate(zip(experiments, COLORS)):\n",
    "        metrics = experiment[\"metrics\"]\n",
    "        plt.plot(\n",
    "            metrics[\"fpr\"], \n",
    "            metrics[\"tpr\"], \n",
    "            label=f\"{experiment['name']} (AUC={metrics['roc_auc']:.3f})\",\n",
    "            color=color, \n",
    "            linestyle=line_styles[idx % len(line_styles)],  # Cycle through line styles\n",
    "            marker=markers[idx % len(markers)],  # Cycle through markers\n",
    "            markevery=0.1,  # Adds markers at regular intervals (every 10% of data points)\n",
    "            linewidth=2,\n",
    "            markersize=10\n",
    "        )\n",
    "        print(f\"{experiment['name']} roc_auc: {metrics['roc_auc']:.3f}\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    \n",
    "    plt.xlabel('False Positive Rate', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate', fontsize=12)\n",
    "    plt.title(f'ROC Curves ({base_model_name} - {args.mask_filling_model_name})', fontsize=14)\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5), fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{SAVE_FOLDER}/roc_curves.png\", dpi=1200)\n",
    "\n",
    "\n",
    "# save the histogram of log likelihoods in two side-by-side plots, one for real and real perturbed, and one for sampled and sampled perturbed\n",
    "def save_ll_histograms(experiments):\n",
    "    # first, clear plt\n",
    "    plt.clf()\n",
    "\n",
    "    for experiment in experiments:\n",
    "        try:\n",
    "            results = experiment[\"raw_results\"]\n",
    "            # plot histogram of sampled/perturbed sampled on left, original/perturbed original on right\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.hist([r[\"sampled_ll\"] for r in results], alpha=0.5, bins='auto', label='sampled')\n",
    "            plt.hist([r[\"perturbed_sampled_ll\"] for r in results], alpha=0.5, bins='auto', label='perturbed sampled')\n",
    "            plt.xlabel(\"log likelihood\")\n",
    "            plt.ylabel('count')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.hist([r[\"original_ll\"] for r in results], alpha=0.5, bins='auto', label='original')\n",
    "            plt.hist([r[\"perturbed_original_ll\"] for r in results], alpha=0.5, bins='auto', label='perturbed original')\n",
    "            plt.xlabel(\"log likelihood\")\n",
    "            plt.ylabel('count')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.savefig(f\"{SAVE_FOLDER}/ll_histograms_{experiment['name']}.png\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "# save the histograms of log likelihood ratios in two side-by-side plots, one for real and real perturbed, and one for sampled and sampled perturbed\n",
    "def save_llr_histograms(experiments):\n",
    "    # first, clear plt\n",
    "    plt.clf()\n",
    "\n",
    "    for experiment in experiments:\n",
    "        try:\n",
    "            results = experiment[\"raw_results\"]\n",
    "            # plot histogram of sampled/perturbed sampled on left, original/perturbed original on right\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "\n",
    "            # compute the log likelihood ratio for each result\n",
    "            for r in results:\n",
    "                r[\"sampled_llr\"] = r[\"sampled_ll\"] - r[\"perturbed_sampled_ll\"]\n",
    "                r[\"original_llr\"] = r[\"original_ll\"] - r[\"perturbed_original_ll\"]\n",
    "            \n",
    "            plt.hist([r[\"sampled_llr\"] for r in results], alpha=0.5, bins='auto', label='sampled')\n",
    "            plt.hist([r[\"original_llr\"] for r in results], alpha=0.5, bins='auto', label='original')\n",
    "            plt.xlabel(\"log likelihood ratio\")\n",
    "            plt.ylabel('count')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.savefig(f\"{SAVE_FOLDER}/llr_histograms_{experiment['name']}.png\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def get_perturbation_results(span_length=10, n_perturbations=1, n_samples=500):\n",
    "    load_mask_model()\n",
    "\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    results = []\n",
    "    original_text = data[\"original\"]\n",
    "    sampled_text = data[\"sampled\"]\n",
    "\n",
    "    perturb_fn = functools.partial(perturb_texts, span_length=span_length, pct=args.pct_words_masked)\n",
    "\n",
    "    p_sampled_text = perturb_fn([x for x in sampled_text for _ in range(n_perturbations)])\n",
    "    p_original_text = perturb_fn([x for x in original_text for _ in range(n_perturbations)])\n",
    "    for _ in range(n_perturbation_rounds - 1):\n",
    "        try:\n",
    "            p_sampled_text, p_original_text = perturb_fn(p_sampled_text), perturb_fn(p_original_text)\n",
    "        except AssertionError:\n",
    "            break\n",
    "\n",
    "    assert len(p_sampled_text) == len(sampled_text) * n_perturbations, f\"Expected {len(sampled_text) * n_perturbations} perturbed samples, got {len(p_sampled_text)}\"\n",
    "    assert len(p_original_text) == len(original_text) * n_perturbations, f\"Expected {len(original_text) * n_perturbations} perturbed samples, got {len(p_original_text)}\"\n",
    "\n",
    "    for idx in range(len(original_text)):\n",
    "        results.append({\n",
    "            \"original\": original_text[idx],\n",
    "            \"sampled\": sampled_text[idx],\n",
    "            \"perturbed_sampled\": p_sampled_text[idx * n_perturbations: (idx + 1) * n_perturbations],\n",
    "            \"perturbed_original\": p_original_text[idx * n_perturbations: (idx + 1) * n_perturbations]\n",
    "        })\n",
    "\n",
    "    load_base_model()\n",
    "\n",
    "    for res in tqdm.tqdm(results, desc=\"Computing log likelihoods\"):\n",
    "        p_sampled_ll = get_lls(res[\"perturbed_sampled\"])\n",
    "        p_original_ll = get_lls(res[\"perturbed_original\"])\n",
    "        res[\"original_ll\"] = get_ll(res[\"original\"])\n",
    "        res[\"sampled_ll\"] = get_ll(res[\"sampled\"])\n",
    "        res[\"all_perturbed_sampled_ll\"] = p_sampled_ll\n",
    "        res[\"all_perturbed_original_ll\"] = p_original_ll\n",
    "        res[\"perturbed_sampled_ll\"] = np.mean(p_sampled_ll)\n",
    "        res[\"perturbed_original_ll\"] = np.mean(p_original_ll)\n",
    "        res[\"perturbed_sampled_ll_std\"] = np.std(p_sampled_ll) if len(p_sampled_ll) > 1 else 1\n",
    "        res[\"perturbed_original_ll_std\"] = np.std(p_original_ll) if len(p_original_ll) > 1 else 1\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_perturbation_experiment(results, criterion, span_length=10, n_perturbations=1, n_samples=500):\n",
    "    # compute diffs with perturbed\n",
    "    predictions = {'real': [], 'samples': []}\n",
    "    for res in results:\n",
    "        if criterion == 'd':\n",
    "            predictions['real'].append(res['original_ll'] - res['perturbed_original_ll'])\n",
    "            predictions['samples'].append(res['sampled_ll'] - res['perturbed_sampled_ll'])\n",
    "        elif criterion == 'z':\n",
    "            if res['perturbed_original_ll_std'] == 0:\n",
    "                res['perturbed_original_ll_std'] = 1\n",
    "                print(\"WARNING: std of perturbed original is 0, setting to 1\")\n",
    "                print(f\"Number of unique perturbed original texts: {len(set(res['perturbed_original']))}\")\n",
    "                print(f\"Original text: {res['original']}\")\n",
    "            if res['perturbed_sampled_ll_std'] == 0:\n",
    "                res['perturbed_sampled_ll_std'] = 1\n",
    "                print(\"WARNING: std of perturbed sampled is 0, setting to 1\")\n",
    "                print(f\"Number of unique perturbed sampled texts: {len(set(res['perturbed_sampled']))}\")\n",
    "                print(f\"Sampled text: {res['sampled']}\")\n",
    "            predictions['real'].append((res['original_ll'] - res['perturbed_original_ll']) / res['perturbed_original_ll_std'])\n",
    "            predictions['samples'].append((res['sampled_ll'] - res['perturbed_sampled_ll']) / res['perturbed_sampled_ll_std'])\n",
    "\n",
    "    fpr, tpr, roc_auc, tp_at_low_fpr = get_roc_metrics(predictions['real'], predictions['samples'], target_fpr=0.01)\n",
    "    p, r, pr_auc = get_precision_recall_metrics(predictions['real'], predictions['samples'])\n",
    "    name = f'perturbation_{n_perturbations}_{criterion}'\n",
    "    print(f\"{name} ROC AUC: {roc_auc}, PR AUC: {pr_auc}\")    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'predictions': predictions,\n",
    "        'info': {\n",
    "            'pct_words_masked': args.pct_words_masked,\n",
    "            'span_length': span_length,\n",
    "            'n_perturbations': n_perturbations,\n",
    "            'n_samples': n_samples,\n",
    "        },\n",
    "        'raw_results': results,\n",
    "        'metrics': {\n",
    "            'roc_auc': roc_auc,\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'tp_at_low_fpr': tp_at_low_fpr  # New key added\n",
    "        },\n",
    "        'pr_metrics': {\n",
    "            'pr_auc': pr_auc,\n",
    "            'precision': p,\n",
    "            'recall': r,\n",
    "        },\n",
    "        'loss': 1 - pr_auc,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_baseline_threshold_experiment(criterion_fn, name, n_samples=500):\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "\n",
    "    results = []\n",
    "    for batch in tqdm.tqdm(range(n_samples // batch_size), desc=f\"Computing {name} criterion\"):\n",
    "        original_text = data[\"original\"][batch * batch_size:(batch + 1) * batch_size]\n",
    "        sampled_text = data[\"sampled\"][batch * batch_size:(batch + 1) * batch_size]\n",
    "\n",
    "        for idx in range(len(original_text)):\n",
    "            results.append({\n",
    "                \"original\": original_text[idx],\n",
    "                \"original_crit\": criterion_fn(original_text[idx]),\n",
    "                \"sampled\": sampled_text[idx],\n",
    "                \"sampled_crit\": criterion_fn(sampled_text[idx]),\n",
    "            })\n",
    "\n",
    "    # compute prediction scores for real/sampled passages\n",
    "    predictions = {\n",
    "        'real': [x[\"original_crit\"] for x in results],\n",
    "        'samples': [x[\"sampled_crit\"] for x in results],\n",
    "    }\n",
    "\n",
    "    fpr, tpr, roc_auc, tp_at_low_fpr = get_roc_metrics(predictions['real'], predictions['samples'], target_fpr=0.01)\n",
    "    p, r, pr_auc = get_precision_recall_metrics(predictions['real'], predictions['samples'])\n",
    "    print(f\"{name}_threshold ROC AUC: {roc_auc}, PR AUC: {pr_auc}, TP@low FPR: {tp_at_low_fpr}\")\n",
    "    return {\n",
    "        'name': f'{name}_threshold',\n",
    "        'predictions': predictions,\n",
    "        'info': {\n",
    "            'n_samples': n_samples,\n",
    "        },\n",
    "        'raw_results': results,\n",
    "        'metrics': {\n",
    "            'roc_auc': roc_auc,\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'tp_at_low_fpr': tp_at_low_fpr  # New key added\n",
    "        },\n",
    "        'pr_metrics': {\n",
    "            'pr_auc': pr_auc,\n",
    "            'precision': p,\n",
    "            'recall': r,\n",
    "        },\n",
    "        'loss': 1 - pr_auc,\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# strip newlines from each example; replace one or more newlines with a single space\n",
    "def strip_newlines(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "\n",
    "# trim to shorter length\n",
    "def trim_to_shorter_length(texta, textb):\n",
    "    # truncate to shorter of o and s\n",
    "    shorter_length = min(len(texta.split(' ')), len(textb.split(' ')))\n",
    "    texta = ' '.join(texta.split(' ')[:shorter_length])\n",
    "    textb = ' '.join(textb.split(' ')[:shorter_length])\n",
    "    return texta, textb\n",
    "\n",
    "\n",
    "def truncate_to_substring(text, substring, idx_occurrence):\n",
    "    # truncate everything after the idx_occurrence occurrence of substring\n",
    "    assert idx_occurrence > 0, 'idx_occurrence must be > 0'\n",
    "    idx = -1\n",
    "    for _ in range(idx_occurrence):\n",
    "        idx = text.find(substring, idx + 1)\n",
    "        if idx == -1:\n",
    "            return text\n",
    "    return text[:idx]\n",
    "\n",
    "\n",
    "def generate_samples(raw_data, batch_size):\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    data = {\n",
    "        \"original\": [],\n",
    "        \"sampled\": [],\n",
    "    }\n",
    "\n",
    "    for batch in range(len(raw_data) // batch_size):\n",
    "        print('Generating samples for batch', batch, 'of', len(raw_data) // batch_size)\n",
    "        original_text = raw_data[batch * batch_size:(batch + 1) * batch_size]\n",
    "        sampled_text = sample_from_model(original_text, min_words=30 if args.dataset in ['pubmed'] else 55)\n",
    "\n",
    "        for o, s in zip(original_text, sampled_text):\n",
    "            if args.dataset == 'pubmed':\n",
    "                s = truncate_to_substring(s, 'Question:', 2)\n",
    "                o = o.replace(custom_datasets.SEPARATOR, ' ')\n",
    "\n",
    "            o, s = trim_to_shorter_length(o, s)\n",
    "\n",
    "            # add to the data\n",
    "            data[\"original\"].append(o)\n",
    "            data[\"sampled\"].append(s)\n",
    "    \n",
    "    if args.pre_perturb_pct > 0:\n",
    "        print(f'APPLYING {args.pre_perturb_pct}, {args.pre_perturb_span_length} PRE-PERTURBATIONS')\n",
    "        load_mask_model()\n",
    "        data[\"sampled\"] = perturb_texts(data[\"sampled\"], args.pre_perturb_span_length, args.pre_perturb_pct, ceil_pct=True)\n",
    "        load_base_model()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_data(dataset, key):\n",
    "    # load data\n",
    "    if dataset in custom_datasets.DATASETS:\n",
    "        data = custom_datasets.load(dataset, cache_dir)\n",
    "    else:\n",
    "        data = datasets.load_dataset(dataset, split='train', cache_dir=cache_dir)[key]\n",
    "\n",
    "    # get unique examples, strip whitespace, and remove newlines\n",
    "    # then take just the long examples, shuffle, take the first 5,000 to tokenize to save time\n",
    "    # then take just the examples that are <= 512 tokens (for the mask model)\n",
    "    # then generate n_samples samples\n",
    "\n",
    "    # remove duplicates from the data\n",
    "    data = list(dict.fromkeys(data))  # deterministic, as opposed to set()\n",
    "\n",
    "    # strip whitespace around each example\n",
    "    data = [x.strip() for x in data]\n",
    "\n",
    "    # remove newlines from each example\n",
    "    data = [strip_newlines(x) for x in data]\n",
    "\n",
    "    # try to keep only examples with > 250 words\n",
    "    if dataset in ['writing', 'squad', 'xsum']:\n",
    "        long_data = [x for x in data if len(x.split()) > 250]\n",
    "        if len(long_data) > 0:\n",
    "            data = long_data\n",
    "\n",
    "    random.seed(0)\n",
    "    random.shuffle(data)\n",
    "\n",
    "    data = data[:5_000]\n",
    "\n",
    "    # keep only examples with <= 512 tokens according to mask_tokenizer\n",
    "    # this step has the extra effect of removing examples with low-quality/garbage content\n",
    "    tokenized_data = preproc_tokenizer(data)\n",
    "    data = [x for x, y in zip(data, tokenized_data[\"input_ids\"]) if len(y) <= 512]\n",
    "\n",
    "    # print stats about remainining data\n",
    "    print(f\"Total number of samples: {len(data)}\")\n",
    "    print(f\"Average number of words: {np.mean([len(x.split()) for x in data])}\")\n",
    "\n",
    "    return generate_samples(data[:n_samples], batch_size=batch_size)\n",
    "\n",
    "\n",
    "def load_base_model_and_tokenizer(name):\n",
    "    if args.openai_model is None:\n",
    "        print(f'Loading BASE model {args.base_model_name}...')\n",
    "        base_model_kwargs = {}\n",
    "        if 'gpt-j' in name or 'neox' in name:\n",
    "            base_model_kwargs.update(dict(torch_dtype=torch.float16))\n",
    "        if 'gpt-j' in name:\n",
    "            base_model_kwargs.update(dict(revision='float16'))\n",
    "        base_model = transformers.AutoModelForCausalLM.from_pretrained(name, **base_model_kwargs, cache_dir=cache_dir)\n",
    "    else:\n",
    "        base_model = None\n",
    "\n",
    "    optional_tok_kwargs = {}\n",
    "    if \"facebook/opt-\" in name:\n",
    "        print(\"Using non-fast tokenizer for OPT\")\n",
    "        optional_tok_kwargs['fast'] = False\n",
    "    if args.dataset in ['pubmed']:\n",
    "        optional_tok_kwargs['padding_side'] = 'left'\n",
    "    base_tokenizer = transformers.AutoTokenizer.from_pretrained(name, **optional_tok_kwargs, cache_dir=cache_dir)\n",
    "    base_tokenizer.pad_token_id = base_tokenizer.eos_token_id\n",
    "\n",
    "    return base_model, base_tokenizer\n",
    "\n",
    "\n",
    "def eval_supervised(data, model):\n",
    "    print(f'Beginning supervised evaluation with {model}...')\n",
    "    detector = transformers.AutoModelForSequenceClassification.from_pretrained(model, cache_dir=cache_dir).to(DEVICE)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model, cache_dir=cache_dir)\n",
    "\n",
    "    real, fake = data['original'], data['sampled']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # get predictions for real\n",
    "        real_preds = []\n",
    "        for batch in tqdm.tqdm(range(len(real) // batch_size), desc=\"Evaluating real\"):\n",
    "            batch_real = real[batch * batch_size:(batch + 1) * batch_size]\n",
    "            batch_real = tokenizer(batch_real, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
    "            real_preds.extend(detector(**batch_real).logits.softmax(-1)[:,0].tolist())\n",
    "        \n",
    "        # get predictions for fake\n",
    "        fake_preds = []\n",
    "        for batch in tqdm.tqdm(range(len(fake) // batch_size), desc=\"Evaluating fake\"):\n",
    "            batch_fake = fake[batch * batch_size:(batch + 1) * batch_size]\n",
    "            batch_fake = tokenizer(batch_fake, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(DEVICE)\n",
    "            fake_preds.extend(detector(**batch_fake).logits.softmax(-1)[:,0].tolist())\n",
    "\n",
    "    predictions = {\n",
    "        'real': real_preds,\n",
    "        'samples': fake_preds,\n",
    "    }\n",
    "\n",
    "    fpr, tpr, roc_auc, tp_at_low_fpr = get_roc_metrics(real_preds, fake_preds, target_fpr=0.01)\n",
    "    p, r, pr_auc = get_precision_recall_metrics(real_preds, fake_preds)\n",
    "    print(f\"{model} ROC AUC: {roc_auc}, PR AUC: {pr_auc}\")\n",
    "\n",
    "    # free GPU memory\n",
    "    del detector\n",
    "    torch.cuda.empty_cache()\n",
    "    return {\n",
    "        'name': model,\n",
    "        'predictions': predictions,\n",
    "        'info': {\n",
    "            'n_samples': n_samples,\n",
    "        },\n",
    "        'metrics': {\n",
    "            'roc_auc': roc_auc,\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'tp_at_low_fpr': tp_at_low_fpr  # New key added\n",
    "        },\n",
    "        'pr_metrics': {\n",
    "            'pr_auc': pr_auc,\n",
    "            'precision': p,\n",
    "            'recall': r,\n",
    "        },\n",
    "        'loss': 1 - pr_auc,\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9475f550-099c-4809-81c4-afb5c4c95a16",
   "metadata": {},
   "source": [
    "# Attribution xsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7ae0f06-cf3a-4f40-867d-5ad75deca4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acb14549678042189b497335181880dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e10aab27cb54228b54d60a000f75c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from tiiuae/falcon-7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d3438bf5984527940fd6c66afd59c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from Qwen/Qwen3-32B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0531690310b8442db07410dfc8e406cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating at fixed prefix length: 300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9818cb867a6401592f4f355e5fd275c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3e3ed513714eb8b94c87a4c7ab5376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304d087ac3fc4c769582375c91d386ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9eced93a27244a0bd4fe810732ee009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error statistics at prefix length 300:\n",
      "meta-llama/Llama-2-7b-chat-hf:\n",
      "  Type I Error Rate: 0.0233\n",
      "  Type II Error Rate: 0.0000\n",
      "meta-llama/Meta-Llama-3-8B:\n",
      "  Type I Error Rate: 0.0000\n",
      "  Type II Error Rate: 0.1420\n",
      "tiiuae/falcon-7b:\n",
      "  Type I Error Rate: 0.0193\n",
      "  Type II Error Rate: 0.0000\n",
      "Qwen/Qwen3-32B:\n",
      "  Type I Error Rate: 0.0047\n",
      "  Type II Error Rate: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Args:\n",
    "    dataset = \"xsum\"\n",
    "    dataset_key = \"document\"\n",
    "    n_samples = 500\n",
    "    batch_size = 50\n",
    "    base_model_name = \"run\"\n",
    "    cache_dir = \"./cache\"\n",
    "    output_name = \"\"\n",
    "    int8 = False\n",
    "    half = True\n",
    "    openai_model = None\n",
    "    openai_key = None\n",
    "\n",
    "args = Args()\n",
    "\n",
    "START_DATE = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "START_TIME = datetime.datetime.now().strftime('%H-%M-%S-%f')\n",
    "\n",
    "base_model_name = args.base_model_name.replace('/', '_')\n",
    "SAVE_FOLDER = f\"./results/{args.output_name}{base_model_name}/{START_DATE}-{START_TIME}\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(SAVE_FOLDER, \"args.json\"), \"w\") as f:\n",
    "    json.dump(args.__dict__, f, indent=4)\n",
    "\n",
    "cache_dir = args.cache_dir\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "def strip_newlines(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def generate_data(dataset, key):\n",
    "    data = load_dataset(dataset, split='train', cache_dir=cache_dir)[key]\n",
    "    data = list(dict.fromkeys([x.strip() for x in data]))\n",
    "    data = [strip_newlines(x) for x in data if len(x.split()) > 250]\n",
    "    random.seed(0)\n",
    "    random.shuffle(data)\n",
    "    return data[:args.n_samples]\n",
    "\n",
    "raw_data = generate_data(args.dataset, args.dataset_key)\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    dtype = torch.float16 if args.half else torch.float32\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_ll(model, tokenizer, text, max_tokens=None):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "    if max_tokens is not None:\n",
    "        inputs['input_ids'] = inputs['input_ids'][:, :max_tokens]\n",
    "        inputs['attention_mask'] = inputs['attention_mask'][:, :max_tokens]\n",
    "\n",
    "    labels = inputs.input_ids.clone()\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    labels = labels[:, 1:]\n",
    "    attention_mask = inputs.attention_mask[:, 1:]\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    ll_per_token = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    ll = (ll_per_token * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return ll.item()\n",
    "\n",
    "def sample_from_model(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "    input_ids = inputs.input_ids[:, :30]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=300,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "model_names = [\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    \"Qwen/Qwen3-32B\"\n",
    "]\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Sampling from {model_name}\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(0, len(raw_data), args.batch_size):\n",
    "        batch = raw_data[i:i + args.batch_size]\n",
    "        gen_texts = sample_from_model(batch, model, tokenizer)\n",
    "        samples.extend(gen_texts)\n",
    "        labels.extend([model_name] * len(gen_texts))\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "with open(os.path.join(SAVE_FOLDER, \"attribution_data.json\"), \"w\") as f:\n",
    "    json.dump({\"samples\": samples, \"labels\": labels}, f)\n",
    "\n",
    "\n",
    "\n",
    "n = 300  # fixed prefix length\n",
    "print(f\"Evaluating at fixed prefix length: {n}\")\n",
    "ll_dict_n = defaultdict(list)\n",
    "\n",
    "for model_name in model_names:\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    model.eval()\n",
    "    for text in samples:\n",
    "        ll = get_ll(model, tokenizer, text, max_tokens=n)\n",
    "        ll_dict_n[model_name].append(ll)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "preds_n = []\n",
    "for i in range(len(samples)):\n",
    "    model_lls = [ll_dict_n[m][i] for m in model_names]\n",
    "    pred = model_names[np.argmax(model_lls)]\n",
    "    preds_n.append(pred)\n",
    "\n",
    "error_stats = {}\n",
    "for model in model_names:\n",
    "    tp = sum((p == l == model) for p, l in zip(preds_n, labels))\n",
    "    fn = sum((p != model and l == model) for p, l in zip(preds_n, labels))\n",
    "    fp = sum((p == model and l != model) for p, l in zip(preds_n, labels))\n",
    "\n",
    "    total_non_model = sum(1 for l in labels if l != model)\n",
    "    total_model = sum(1 for l in labels if l == model)\n",
    "\n",
    "    type_i_rate = fp / total_non_model if total_non_model else 0\n",
    "    type_ii_rate = fn / total_model if total_model else 0\n",
    "\n",
    "    error_stats[model] = {'type_i': type_i_rate, 'type_ii': type_ii_rate}\n",
    "\n",
    "# Report results\n",
    "print(f\"\\nError statistics at prefix length {n}:\")\n",
    "for model in model_names:\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Type I Error Rate: {error_stats[model]['type_i']:.4f}\")\n",
    "    print(f\"  Type II Error Rate: {error_stats[model]['type_ii']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28d5f71-148a-4b21-8a02-1c9e47e157ea",
   "metadata": {},
   "source": [
    "# Attribution squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "927f9785-1705-4547-a3e3-ec1db5a2d729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2740e5373344f47ab0d4034e7847fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ce096d379b414a932b485d64add771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from tiiuae/falcon-7b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e4b36200bea46df9b7f05348675132b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from Qwen/Qwen3-32B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cae56345ff041aea42349b0bd1a2cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating at fixed prefix length: 300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70c72cb0a08494d99c0865bd777a172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f36eacd6ff49b4b76a9c8b14e0eed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e286a72b73c94859b59c17764eb92b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768284407bce4c2c9cb5aa15230c510a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error statistics at prefix length 300:\n",
      "meta-llama/Llama-2-7b-chat-hf:\n",
      "  Type I Error Rate: 0.0186\n",
      "  Type II Error Rate: 0.0000\n",
      "meta-llama/Meta-Llama-3-8B:\n",
      "  Type I Error Rate: 0.0000\n",
      "  Type II Error Rate: 0.1006\n",
      "tiiuae/falcon-7b:\n",
      "  Type I Error Rate: 0.0037\n",
      "  Type II Error Rate: 0.0363\n",
      "Qwen/Qwen3-32B:\n",
      "  Type I Error Rate: 0.0233\n",
      "  Type II Error Rate: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import defaultdict\n",
    "from datasets import load_dataset\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Args:\n",
    "    dataset = \"squad\"\n",
    "    dataset_key = \"context\"\n",
    "    n_samples = 500\n",
    "    batch_size = 50\n",
    "    base_model_name = \"run\"\n",
    "    cache_dir = \"./cache\"\n",
    "    output_name = \"\"\n",
    "    int8 = False\n",
    "    half = True\n",
    "    openai_model = None\n",
    "    openai_key = None\n",
    "\n",
    "args = Args()\n",
    "\n",
    "START_DATE = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "START_TIME = datetime.datetime.now().strftime('%H-%M-%S-%f')\n",
    "\n",
    "base_model_name = args.base_model_name.replace('/', '_')\n",
    "SAVE_FOLDER = f\"./results/{args.output_name}{base_model_name}/{START_DATE}-{START_TIME}\"\n",
    "os.makedirs(SAVE_FOLDER, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(SAVE_FOLDER, \"args.json\"), \"w\") as f:\n",
    "    json.dump(args.__dict__, f, indent=4)\n",
    "\n",
    "cache_dir = args.cache_dir\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "def strip_newlines(text):\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def generate_data(dataset, key):\n",
    "    data = load_dataset(dataset, split='train', cache_dir=cache_dir)[key]\n",
    "    data = list(dict.fromkeys([x.strip() for x in data]))\n",
    "    data = [strip_newlines(x) for x in data if len(x.split()) > 250]\n",
    "    random.seed(0)\n",
    "    random.shuffle(data)\n",
    "    return data[:args.n_samples]\n",
    "\n",
    "raw_data = generate_data(args.dataset, args.dataset_key)\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    dtype = torch.float16 if args.half else torch.float32\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir=cache_dir,\n",
    "        torch_dtype=dtype,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def get_ll(model, tokenizer, text, max_tokens=None):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "    if max_tokens is not None:\n",
    "        inputs['input_ids'] = inputs['input_ids'][:, :max_tokens]\n",
    "        inputs['attention_mask'] = inputs['attention_mask'][:, :max_tokens]\n",
    "\n",
    "    labels = inputs.input_ids.clone()\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits[:, :-1, :]\n",
    "    labels = labels[:, 1:]\n",
    "    attention_mask = inputs.attention_mask[:, 1:]\n",
    "    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    ll_per_token = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
    "    ll = (ll_per_token * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return ll.item()\n",
    "\n",
    "def sample_from_model(texts, model, tokenizer):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "    input_ids = inputs.input_ids[:, :30]\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    outputs = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=300,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "model_names = [\n",
    "    \"meta-llama/Llama-2-7b-chat-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"tiiuae/falcon-7b\",\n",
    "    \"Qwen/Qwen3-32B\"\n",
    "]\n",
    "\n",
    "samples = []\n",
    "labels = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Sampling from {model_name}\")\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    for i in range(0, len(raw_data), args.batch_size):\n",
    "        batch = raw_data[i:i + args.batch_size]\n",
    "        gen_texts = sample_from_model(batch, model, tokenizer)\n",
    "        samples.extend(gen_texts)\n",
    "        labels.extend([model_name] * len(gen_texts))\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "with open(os.path.join(SAVE_FOLDER, \"attribution_data.json\"), \"w\") as f:\n",
    "    json.dump({\"samples\": samples, \"labels\": labels}, f)\n",
    "\n",
    "\n",
    "\n",
    "n = 300  # fixed prefix length\n",
    "print(f\"Evaluating at fixed prefix length: {n}\")\n",
    "ll_dict_n = defaultdict(list)\n",
    "\n",
    "for model_name in model_names:\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "    model.eval()\n",
    "    for text in samples:\n",
    "        ll = get_ll(model, tokenizer, text, max_tokens=n)\n",
    "        ll_dict_n[model_name].append(ll)\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "preds_n = []\n",
    "for i in range(len(samples)):\n",
    "    model_lls = [ll_dict_n[m][i] for m in model_names]\n",
    "    pred = model_names[np.argmax(model_lls)]\n",
    "    preds_n.append(pred)\n",
    "\n",
    "error_stats = {}\n",
    "for model in model_names:\n",
    "    tp = sum((p == l == model) for p, l in zip(preds_n, labels))\n",
    "    fn = sum((p != model and l == model) for p, l in zip(preds_n, labels))\n",
    "    fp = sum((p == model and l != model) for p, l in zip(preds_n, labels))\n",
    "\n",
    "    total_non_model = sum(1 for l in labels if l != model)\n",
    "    total_model = sum(1 for l in labels if l == model)\n",
    "\n",
    "    type_i_rate = fp / total_non_model if total_non_model else 0\n",
    "    type_ii_rate = fn / total_model if total_model else 0\n",
    "\n",
    "    error_stats[model] = {'type_i': type_i_rate, 'type_ii': type_ii_rate}\n",
    "\n",
    "# Report results\n",
    "print(f\"\\nError statistics at prefix length {n}:\")\n",
    "for model in model_names:\n",
    "    print(f\"{model}:\")\n",
    "    print(f\"  Type I Error Rate: {error_stats[model]['type_i']:.4f}\")\n",
    "    print(f\"  Type II Error Rate: {error_stats[model]['type_ii']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34925bc5-73bd-41ee-90dc-27b0ecade5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
